{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m output_video \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mordered_aligned_video.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     40\u001b[0m image_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.png\u001b[39m\u001b[38;5;124m'\u001b[39m)))  \u001b[38;5;66;03m# Adjust if needed\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage_files\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m images:\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo images loaded. Check the folder path or file extensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 41\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     39\u001b[0m output_video \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mordered_aligned_video.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     40\u001b[0m image_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.png\u001b[39m\u001b[38;5;124m'\u001b[39m)))  \u001b[38;5;66;03m# Adjust if needed\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m images \u001b[38;5;241m=\u001b[39m [\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m image_files \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mimread(img) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m images:\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo images loaded. Check the folder path or file extensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "import glob\n",
    "import os\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from tqdm import tqdm  # Use `from tqdm.notebook import tqdm` if you're in a notebook environment\n",
    "\n",
    "def calculate_similarity(image1, image2):\n",
    "    if image1.shape != image2.shape:\n",
    "        image2 = cv2.resize(image2, (image1.shape[1], image1.shape[0]))\n",
    "    gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "    gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "    score, _ = ssim(gray1, gray2, full=True)\n",
    "    return score\n",
    "\n",
    "def align_images(base_img, img_to_align):\n",
    "    base_gray = cv2.cvtColor(base_img, cv2.COLOR_BGR2GRAY)\n",
    "    align_gray = cv2.cvtColor(img_to_align, cv2.COLOR_BGR2GRAY)\n",
    "    orb = cv2.ORB_create()\n",
    "    keypoints1, descriptors1 = orb.detectAndCompute(base_gray, None)\n",
    "    keypoints2, descriptors2 = orb.detectAndCompute(align_gray, None)\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(descriptors1, descriptors2)\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "    points1 = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    points2 = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    matrix, _ = cv2.findHomography(points2, points1, cv2.RANSAC, 5.0)\n",
    "    height, width = base_img.shape[:2]\n",
    "    aligned_image = cv2.warpPerspective(img_to_align, matrix, (width, height))\n",
    "    return aligned_image\n",
    "\n",
    "def create_video_from_frames(frames, output_filename, fps=30):\n",
    "    clip = ImageSequenceClip(frames, fps=fps)\n",
    "    clip.write_videofile(output_filename, codec=\"libx264\")\n",
    "\n",
    "# Folder containing the images\n",
    "image_folder = r'D:\\GenV2\\train\\images'\n",
    "output_video = r'ordered_aligned_video.mp4'\n",
    "image_files = sorted(glob.glob(os.path.join(image_folder, '*.png')))  # Adjust if needed\n",
    "images = [cv2.imread(img) for img in image_files if cv2.imread(img) is not None]\n",
    "\n",
    "if not images:\n",
    "    raise ValueError(\"No images loaded. Check the folder path or file extensions.\")\n",
    "\n",
    "# Compute similarity matrix with progress tracking\n",
    "n = len(images)\n",
    "similarity_matrix = np.zeros((n, n))\n",
    "print(\"Computing similarity matrix:\")\n",
    "for i in tqdm(range(n), desc=\"Rows\"):\n",
    "    for j in range(i + 1, n):\n",
    "        similarity_matrix[i, j] = calculate_similarity(images[i], images[j])\n",
    "        similarity_matrix[j, i] = similarity_matrix[i, j]\n",
    "\n",
    "# Find the sequence of images based on maximum similarity\n",
    "sequence = [0]  # Start with the first image\n",
    "used = set(sequence)\n",
    "print(\"Ordering images by similarity:\")\n",
    "for _ in tqdm(range(1, n), desc=\"Sequence\"):\n",
    "    last = sequence[-1]\n",
    "    try:\n",
    "        next_image = max(\n",
    "            [(j, similarity_matrix[last, j]) for j in range(n) if j not in used],\n",
    "            key=lambda x: x[1]\n",
    "        )[0]\n",
    "    except ValueError:\n",
    "        print(\"Error finding next image. Check similarity scores.\")\n",
    "        break\n",
    "    sequence.append(next_image)\n",
    "    used.add(next_image)\n",
    "\n",
    "print(\"Sequence order based on similarity:\", sequence)\n",
    "\n",
    "# Arrange and align images according to the computed sequence with progress tracking\n",
    "ordered_aligned_images = [images[sequence[0]]]\n",
    "print(\"Aligning images:\")\n",
    "for i in tqdm(range(1, len(sequence)), desc=\"Aligning\"):\n",
    "    aligned_img = align_images(ordered_aligned_images[-1], images[sequence[i]])\n",
    "    ordered_aligned_images.append(aligned_img)\n",
    "\n",
    "# Create video\n",
    "create_video_from_frames(ordered_aligned_images, output_video, fps=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 64\u001b[0m\n\u001b[0;32m     61\u001b[0m output_video \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mordered_aligned_video.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     63\u001b[0m image_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.png\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m---> 64\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage_files\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m images:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo images loaded. Check the folder path or file extensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 64\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     61\u001b[0m output_video \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mordered_aligned_video.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     63\u001b[0m image_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.png\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m---> 64\u001b[0m images \u001b[38;5;241m=\u001b[39m [\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m image_files \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mimread(img) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m images:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo images loaded. Check the folder path or file extensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "import glob\n",
    "import os\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def calculate_similarity(image1, image2):\n",
    "    if image1.shape != image2.shape:\n",
    "        image2 = cv2.resize(image2, (image1.shape[1], image1.shape[0]))\n",
    "    \n",
    "    gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "    gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    score, _ = ssim(gray1, gray2, full=True)\n",
    "    return score\n",
    "\n",
    "def align_images(base_img, img_to_align):\n",
    "    base_gray = cv2.cvtColor(base_img, cv2.COLOR_BGR2GRAY)\n",
    "    align_gray = cv2.cvtColor(img_to_align, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    orb = cv2.ORB_create()\n",
    "    keypoints1, descriptors1 = orb.detectAndCompute(base_gray, None)\n",
    "    keypoints2, descriptors2 = orb.detectAndCompute(align_gray, None)\n",
    "    \n",
    "    if descriptors1 is None or descriptors2 is None:\n",
    "        return img_to_align\n",
    "    \n",
    "    # Use FLANN matcher\n",
    "    index_params = dict(algorithm=6, table_number=6, key_size=12, multi_probe_level=1)\n",
    "    search_params = dict(checks=50)\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    matches = flann.knnMatch(descriptors1, descriptors2, k=2)\n",
    "    \n",
    "    # Lowe's ratio test\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.7 * n.distance:\n",
    "            good_matches.append(m)\n",
    "    \n",
    "    if len(good_matches) < 4:\n",
    "        return img_to_align\n",
    "    \n",
    "    points1 = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    points2 = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    \n",
    "    matrix, _ = cv2.findHomography(points2, points1, cv2.RANSAC, 5.0)\n",
    "    height, width = base_img.shape[:2]\n",
    "    aligned_image = cv2.warpPerspective(img_to_align, matrix, (width, height))\n",
    "    \n",
    "    return aligned_image\n",
    "\n",
    "def create_video_from_frames(frames, output_filename, fps=30):\n",
    "    clip = ImageSequenceClip(frames, fps=fps)\n",
    "    clip.write_videofile(output_filename, codec=\"libx264\")\n",
    "\n",
    "# Folder containing the images\n",
    "image_folder = r'D:\\GenV2\\train\\images'\n",
    "output_video = r'ordered_aligned_video.mp4'\n",
    "\n",
    "image_files = sorted(glob.glob(os.path.join(image_folder, '*.png')))\n",
    "images = [cv2.imread(img) for img in image_files if cv2.imread(img) is not None]\n",
    "\n",
    "if not images:\n",
    "    raise ValueError(\"No images loaded. Check the folder path or file extensions.\")\n",
    "\n",
    "n = len(images)\n",
    "similarity_matrix = np.zeros((n, n))\n",
    "\n",
    "# Parallel similarity computation\n",
    "def compute_similarity(i, j):\n",
    "    return calculate_similarity(images[i], images[j])\n",
    "\n",
    "with Parallel(n_jobs=-1) as parallel:\n",
    "    results = parallel(delayed(compute_similarity)(i, j) for i in range(n) for j in range(i + 1, n))\n",
    "\n",
    "# Fill the similarity matrix from the results\n",
    "index = 0\n",
    "for i in range(n):\n",
    "    for j in range(i + 1, n):\n",
    "        similarity_matrix[i, j] = results[index]\n",
    "        similarity_matrix[j, i] = similarity_matrix[i, j]\n",
    "        index += 1\n",
    "\n",
    "# Find sequence based on maximum similarity\n",
    "sequence = [0]\n",
    "used = set(sequence)\n",
    "for _ in tqdm(range(1, n), desc=\"Ordering images by similarity\"):\n",
    "    last = sequence[-1]\n",
    "    try:\n",
    "        next_image = max(\n",
    "            [(j, similarity_matrix[last, j]) for j in range(n) if j not in used],\n",
    "            key=lambda x: x[1]\n",
    "        )[0]\n",
    "    except ValueError:\n",
    "        print(\"Error finding next image. Check similarity scores.\")\n",
    "        break\n",
    "    sequence.append(next_image)\n",
    "    used.add(next_image)\n",
    "\n",
    "# Arrange and align images based on computed sequence\n",
    "ordered_aligned_images = [images[sequence[0]]]\n",
    "for i in tqdm(range(1, len(sequence)), desc=\"Aligning images\"):\n",
    "    aligned_img = align_images(ordered_aligned_images[-1], images[sequence[i]])\n",
    "    ordered_aligned_images.append(aligned_img)\n",
    "\n",
    "# Create video\n",
    "create_video_from_frames(ordered_aligned_images, output_video, fps=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\GenV2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
